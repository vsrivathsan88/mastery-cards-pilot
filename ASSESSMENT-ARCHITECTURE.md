# Assessment-First Architecture

## üéØ Overview

This document describes the **assessment-first tool architecture** that ensures robust, consistent student evaluation in the Mastery Cards app.

## üèóÔ∏è Architecture Pattern

### The Problem We Solved

**Before (Direct Action):**
```
Student answers ‚Üí Pi decides ‚Üí Pi awards points ‚Üí Pi advances card
                     ‚ö†Ô∏è Inconsistent    ‚ö†Ô∏è Too eager    ‚ö†Ô∏è No validation
```

**Issues:**
- Pi awarded points for lucky guesses
- Pi moved on too quickly (after 1 turn)
- No consistent criteria applied
- Minimal responses ("yeah", "ok") got full credit
- Repetitive answers weren't detected
- Hard to debug why decisions were made

---

### After (Assessment-First)

```
Student answers ‚Üí Pi calls check_mastery_understanding()
                     ‚Üì
                  Tool analyzes against criteria
                     ‚Üì
              Returns: hasMastery + reasoning
                     ‚Üì
         If TRUE ‚Üí Pi awards points ‚Üí Pi calls should_advance_card()
                                          ‚Üì
                                  Tool validates readiness
                                          ‚Üì
                                  Returns: shouldAdvance
                                          ‚Üì
                              If TRUE ‚Üí Pi advances card
```

**Benefits:**
- ‚úÖ Consistent assessment using card-specific criteria
- ‚úÖ Detects guessing, minimal responses, repetition
- ‚úÖ Validates timing (turn count, time on card)
- ‚úÖ Clear reasoning for every decision
- ‚úÖ Debuggable (all assessments logged)
- ‚úÖ Upgradeable (can swap in ML models later)

---

## üî¨ Tool 1: check_mastery_understanding

### Purpose
Analyzes student response against card-specific mastery criteria **before** awarding points.

### Signature
```typescript
check_mastery_understanding(
  studentResponse: string,    // Exact student text
  cardId: string,              // Current card ID
  milestoneType: 'basic' | 'advanced' | 'teaching'
)
```

### Returns
```typescript
{
  hasMastery: boolean,          // True if student demonstrated mastery
  confidence: number,           // 0-1 score (0.7+ recommended for passing)
  depth: 'surface' | 'partial' | 'deep',
  reasoning: string,            // Human-readable explanation
  suggestedPoints: number,      // Points to award if hasMastery is true
  matchedConcepts: string[],    // Keywords they mentioned
  missingConcepts: string[]     // Keywords they didn't mention
}
```

### Analysis Logic

#### 1. **Minimal Response Detection**
```typescript
Patterns: /^(yeah|yep|ok|okay|uh-huh|mm-hmm|sure|yes|no|maybe|idk|i guess)$/i

Result: hasMastery = false, confidence = 0.1
Reasoning: "Minimal response - needs elaboration"
```

#### 2. **Uncertainty Detection**
```typescript
Checks for:
- Question marks: "Four?"
- Hedging: "I think", "maybe", "probably", "kinda", "sorta"

Result: hasMastery = false, confidence = 0.3
Reasoning: "Response shows uncertainty - student needs to articulate with confidence"
```

#### 3. **Keyword Coverage Analysis**
```typescript
Uses milestone.evidenceKeywords from card data
Calculates: matchedKeywords.length / totalKeywords

Coverage < 30%: hasMastery = false, confidence = 0.4
Coverage >= 50% + depth: hasMastery = true, confidence = 0.7-0.9
```

Example for Card 1 (Equal Cookies):
```
evidenceKeywords: ["four", "4", "equal", "same size", "identical", "same"]

Student: "Four cookies" 
‚Üí Coverage: 1/6 = 16.7% ‚Üí FAIL

Student: "Four equal cookies"
‚Üí Coverage: 2/6 = 33% ‚Üí PARTIAL (needs follow-up)

Student: "Four cookies that are all the same size"
‚Üí Coverage: 3/6 = 50% + depth ‚Üí PASS
```

#### 4. **Depth Analysis**
```typescript
Word count >= 4: Shows thought process
Word count >= 8: Deep explanation

Combined with keyword coverage:
- High coverage + low depth = Parroting (FAIL)
- High coverage + high depth = True understanding (PASS)
```

#### 5. **Repetition Detection**
```typescript
Checks last 3 student responses
If current response matches previous: hasMastery = false, confidence = 0.3
Reasoning: "Student is repeating - may be guessing"
```

#### 6. **Turn Count Validation**
```typescript
Basic/Advanced milestones: Requires 2+ turns
Teaching milestones: Requires 3+ turns

If insufficient turns: hasMastery = false
Reasoning: "Only X turns - need at least Y to verify understanding"
```

### Confidence Score Guide

| Score | Meaning | Action |
|-------|---------|--------|
| 0.9+ | Deep understanding, full concept coverage | Award points immediately |
| 0.7-0.9 | Strong understanding, meets criteria | Award points |
| 0.5-0.7 | Partial understanding | Ask follow-up question |
| 0.3-0.5 | Surface answer or uncertain | Ask clarifying question |
| <0.3 | Minimal/guessing/repetition | Try different approach |

### Example Flow

**Scenario: Card 1 - Equal Cookies (Basic Milestone)**

```
Pi: "What do you notice about these cookies?"
Student: "Four"

Pi calls: check_mastery_understanding(
  studentResponse: "Four",
  cardId: "card-1-cookies",
  milestoneType: "basic"
)

Tool analyzes:
- evidenceKeywords: ["four", "4", "equal", "same size", "identical", "same"]
- Matched: ["four"] ‚Üí 16.7% coverage
- Word count: 1 ‚Üí No depth
- Result: hasMastery = false, confidence = 0.4

Tool returns:
{
  hasMastery: false,
  confidence: 0.4,
  depth: 'surface',
  reasoning: "Only 16% keyword match. Missing: equal, same size, identical, same",
  suggestedPoints: 0,
  matchedConcepts: ["four"],
  missingConcepts: ["equal", "same size", "identical", "same"]
}

Pi sees hasMastery: false ‚Üí Asks follow-up:
"Okay! Tell me more about those cookies - what else do you notice?"

Student: "They're all the same size"

Pi calls: check_mastery_understanding(
  studentResponse: "They're all the same size",
  cardId: "card-1-cookies",
  milestoneType: "basic"
)

Tool analyzes:
- Matched: ["same size", "same"] ‚Üí 33% coverage
- Word count: 6 ‚Üí Has depth
- Combined with previous response: ["four", "same size"] ‚Üí 50% total
- Result: hasMastery = true, confidence = 0.85

Tool returns:
{
  hasMastery: true,
  confidence: 0.85,
  depth: 'partial',
  reasoning: "Strong response with 50% concept coverage and 6 words of explanation. Matched: same size, same",
  suggestedPoints: 30,
  matchedConcepts: ["same size", "same"],
  missingConcepts: ["equal", "identical"]
}

Pi sees hasMastery: true ‚Üí Awards points:
"Yes! They're all the same size! Perfect!"
award_mastery_points(cardId: "card-1-cookies", points: 30, celebration: "Great observation!")
```

---

## üîç Tool 2: should_advance_card

### Purpose
Validates whether it's appropriate to move to the next card **before** advancing.

### Signature
```typescript
should_advance_card(
  cardId: string,
  reason: 'mastered' | 'struggling' | 'incomplete'
)
```

### Returns
```typescript
{
  shouldAdvance: boolean,       // True if ready to advance
  feedback: string,             // Reasoning for decision
  conversationTurns: number,    // Current turn count
  timeSinceCardChange: number,  // Seconds on this card
  currentCardId: string         // Validation check
}
```

### Decision Logic

#### Reason: 'mastered'
```typescript
if (conversationTurns < 2 && timeSinceCardChange > 2s && cardNumber > 0):
  shouldAdvance = false
  feedback = "Cannot advance yet - only X turns. Ask challenge questions."
else:
  shouldAdvance = true
  feedback = "Good to advance - demonstrated understanding through X turns"
```

#### Reason: 'struggling'
```typescript
if (conversationTurns >= 3):
  shouldAdvance = true
  feedback = "Student tried X times. Moving on is appropriate."
else:
  shouldAdvance = false
  feedback = "Give more attempts. Try rephrasing question."
```

#### Reason: 'incomplete'
```typescript
shouldAdvance = false
feedback = "Student hasn't fully engaged. Ask starting question or follow-up."
```

### Example Flow

```
Pi thinks student has mastered card

Pi calls: should_advance_card(
  cardId: "card-1-cookies",
  reason: "mastered"
)

Tool checks:
- conversationTurns: 3
- timeSinceCardChange: 15 seconds
- cardNumber: 1 (not welcome card)

Tool returns:
{
  shouldAdvance: true,
  feedback: "Good to advance - student demonstrated understanding through 3 turns",
  conversationTurns: 3,
  timeSinceCardChange: 15,
  currentCardId: "card-1-cookies"
}

Pi sees shouldAdvance: true ‚Üí Calls show_next_card()
```

---

## üìä Complete Workflow Example

### Happy Path (Student Understands)

```
Card 1: Equal Cookies

1. Pi: "What do you notice about these cookies?"

2. Student: "Four cookies"

3. Pi calls: check_mastery_understanding("Four cookies", "card-1-cookies", "basic")
   Returns: { hasMastery: false, reasoning: "Missing 'equal' concept" }

4. Pi: "Nice! What can you tell me about those four cookies?"

5. Student: "They're all the same size"

6. Pi calls: check_mastery_understanding("They're all the same size", "card-1-cookies", "basic")
   Returns: { hasMastery: true, confidence: 0.85, suggestedPoints: 30 }

7. Pi: "Yes! That's exactly right - four equal cookies!"

8. Pi calls: award_mastery_points("card-1-cookies", 30, "Perfect observation!")
   ‚Üí Points awarded, UI updates

9. Pi calls: should_advance_card("card-1-cookies", "mastered")
   Returns: { shouldAdvance: true, feedback: "Good to advance - 2 turns" }

10. Pi calls: show_next_card()
    ‚Üí Card advances to Card 4

11. Pi: "Next one! This brownie was split..."
```

### Struggling Path (Student Doesn't Understand)

```
Card 1: Equal Cookies

1. Pi: "What do you notice about these cookies?"

2. Student: "Cookies"

3. Pi calls: check_mastery_understanding("Cookies", "card-1-cookies", "basic")
   Returns: { hasMastery: false, confidence: 0.2, reasoning: "Minimal response" }

4. Pi: "Tell me more - what do you see?"

5. Student: "They're round"

6. Pi calls: check_mastery_understanding("They're round", "card-1-cookies", "basic")
   Returns: { hasMastery: false, confidence: 0.3, reasoning: "Doesn't mention key concepts" }

7. Pi: "Look at how many there are - anything else?"

8. Student: "I don't know"

9. Pi calls: should_advance_card("card-1-cookies", "struggling")
   Returns: { shouldAdvance: true, feedback: "Student tried 3 times - move on" }

10. Pi: "No worries! Let's try a different one"

11. Pi calls: show_next_card()
    ‚Üí Card advances (no points awarded)
```

---

## üîß Technical Implementation

### Tool Handler Location
```typescript
// src/App.tsx - handleToolCall callback

case 'check_mastery_understanding': {
  const { studentResponse, cardId, milestoneType } = args;
  
  // Get current card and milestone
  const card = currentCard;
  const milestone = milestoneType === 'teaching' 
    ? card.misconception?.teachingMilestone
    : card.milestones[milestoneType];
  
  // Run analysis algorithms...
  
  // Return assessment to Pi
  response.response = { result: JSON.stringify(assessmentResult) };
  response.scheduling = 'SILENT';  // Internal feedback
}
```

### Feedback Loop

1. **Pi calls assessment tool** ‚Üí Gemini Live API sends function call
2. **Tool handler executes** ‚Üí Runs analysis synchronously
3. **Returns structured JSON** ‚Üí Sent back via Live API
4. **Pi receives result** ‚Üí Parses JSON and adjusts teaching strategy
5. **Logged to transcript** ‚Üí Saved for debugging and analysis

### Data Sources

Assessment uses existing card data:
```typescript
// From mvp-cards-data.ts
card.milestones.basic = {
  points: 30,
  description: "Student mentions both: (1) the number 4, AND (2) equal/same size",
  evidenceKeywords: ["four", "4", "equal", "same size", "identical", "same"]
}
```

### Performance

- Analysis runs synchronously in <1ms
- No external API calls
- No blocking operations
- Returns immediately to Pi

---

## üìà Benefits Recap

### 1. **Consistent Assessment**
- Same criteria applied to every student
- No variability in Pi's judgment
- Objective scoring based on evidence

### 2. **Gaming Prevention**
- Detects minimal responses ("yeah", "ok")
- Catches repetition (saying same thing repeatedly)
- Identifies guessing (uncertainty markers)
- Requires depth (word count, explanation)

### 3. **Sophisticated Analysis**
- Keyword matching (concept coverage)
- Depth analysis (word count, elaboration)
- Context awareness (conversation history)
- Timing validation (turn count, time on card)

### 4. **Clear Debugging**
- Every assessment logged with reasoning
- Confidence scores show certainty
- Matched/missing concepts listed
- Full transcript preserved

### 5. **Upgradeable**
- Can swap in ML models for semantic analysis
- Can add sentiment detection
- Can incorporate misconception detection
- Can integrate with teacher dashboard

### 6. **Teacher Dashboard Ready**
- All assessment data captured
- Confidence scores available
- Concept coverage tracked
- Time on task measured

---

## üöÄ Future Enhancements

### Phase 2: ML-Powered Assessment
```typescript
// Use GPT-4 for semantic understanding
const semanticAnalysis = await openai.chat.completions.create({
  model: "gpt-4",
  messages: [{
    role: "system",
    content: `Analyze if student understands ${concept}. 
    Expected: ${milestone.description}
    Student said: "${studentResponse}"`
  }]
});
```

### Phase 3: Misconception Detection
```typescript
// Detect specific misconceptions
const misconceptions = [
  "Thinks denominator shows how many you have (not how many parts)",
  "Thinks bigger denominator = bigger fraction",
  "Doesn't understand need for equal parts"
];

// Return which misconception student exhibited
```

### Phase 4: Adaptive Difficulty
```typescript
// Adjust card difficulty based on performance
if (averageConfidence > 0.9 && fastResponseTime) {
  recommendDifficulty = 'advanced';
} else if (averageConfidence < 0.5 && slowResponseTime) {
  recommendDifficulty = 'remedial';
}
```

---

## üìù Summary

The **assessment-first architecture** transforms mastery evaluation from an ad-hoc Pi decision into a robust, validated, debuggable system that:

1. ‚úÖ Ensures consistent assessment across all students
2. ‚úÖ Prevents gaming and minimal responses
3. ‚úÖ Provides clear reasoning for every decision
4. ‚úÖ Enables teacher dashboards and analytics
5. ‚úÖ Maintains natural conversation flow (SILENT scheduling)
6. ‚úÖ Is upgradeable to ML-powered analysis

This architecture is production-ready for classroom pilots and can scale to thousands of students with consistent quality.

**GitHub Repo:** https://github.com/vsrivathsan88/mastery-cards-pilot
**Commit:** feat: Implement assessment-first architecture with validation tools
